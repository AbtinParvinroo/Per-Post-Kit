{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b1b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class InterpretabilityExplainer:\n",
    "    \"\"\"\n",
    "    کلاس تفسیرپذیری برای مدل.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    method : str\n",
    "        نام متد: 'SHAPExplainer', 'LIMEExplainer', 'FeatureImportance', 'PartialDependence'\n",
    "    config : dict\n",
    "        تنظیمات مربوط به متد انتخابی.\n",
    "    \"\"\"\n",
    "    def __init__(self, method, config):\n",
    "        self.method = method\n",
    "        self.config = config\n",
    "        self.explainer_ = None\n",
    "\n",
    "    def transform(self, dataset, model):\n",
    "        \"\"\"\n",
    "        اعمال متد تفسیرپذیری روی دیتاست.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : pd.DataFrame\n",
    "            دیتاست ورودی (شامل ویژگی‌ها، احتمالاً بدون هدف).\n",
    "        model : fitted model\n",
    "            مدل آموزش‌دیده.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : pd.DataFrame\n",
    "            دیتاست با ستون‌های اضافی حاوی نتایج تفسیر.\n",
    "        \"\"\"\n",
    "        if not isinstance(dataset, pd.DataFrame):\n",
    "            raise TypeError(\"dataset must be a pandas DataFrame\")\n",
    "\n",
    "        X = dataset.copy()\n",
    "\n",
    "        if self.method == 'SHAPExplainer':\n",
    "            return self._shap_explain(X, model)\n",
    "        elif self.method == 'LIMEExplainer':\n",
    "            return self._lime_explain(X, model)\n",
    "        elif self.method == 'FeatureImportance':\n",
    "            return self._feature_importance(X, model)\n",
    "        elif self.method == 'PartialDependence':\n",
    "            return self._partial_dependence(X, model)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown interpretability method: {self.method}\")\n",
    "\n",
    "    def _shap_explain(self, X, model):\n",
    "        \"\"\"محاسبه مقادیر SHAP و اضافه کردن به دیتاست.\"\"\"\n",
    "        try:\n",
    "            import shap\n",
    "        except ImportError:\n",
    "            raise ImportError(\"shap package is required for SHAPExplainer.\")\n",
    "\n",
    "        explainer_type = self.config.get('explainer_type', None)\n",
    "        if explainer_type is None:\n",
    "            # تشخیص خودکار نوع explainer\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                if hasattr(model, 'tree_'):\n",
    "                    explainer = shap.TreeExplainer(model)\n",
    "                elif hasattr(model, 'coef_'):\n",
    "                    explainer = shap.LinearExplainer(model, X)\n",
    "                else:\n",
    "                    explainer = shap.KernelExplainer(model.predict_proba, X)\n",
    "            else:\n",
    "                if hasattr(model, 'tree_'):\n",
    "                    explainer = shap.TreeExplainer(model)\n",
    "                elif hasattr(model, 'coef_'):\n",
    "                    explainer = shap.LinearExplainer(model, X)\n",
    "                else:\n",
    "                    explainer = shap.KernelExplainer(model.predict, X)\n",
    "        else:\n",
    "            if explainer_type == 'tree':\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "            elif explainer_type == 'linear':\n",
    "                explainer = shap.LinearExplainer(model, X)\n",
    "            elif explainer_type == 'kernel':\n",
    "                pred_fn = model.predict_proba if hasattr(model, 'predict_proba') else model.predict\n",
    "                explainer = shap.KernelExplainer(pred_fn, X)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported explainer_type: {explainer_type}\")\n",
    "\n",
    "        shap_values = explainer.shap_values(X)\n",
    "\n",
    "        if isinstance(shap_values, list):\n",
    "            class_index = self.config.get('class_index', 0)\n",
    "            shap_vals = shap_values[class_index]\n",
    "        else:\n",
    "            shap_vals = shap_values\n",
    "\n",
    "        shap_cols = [f\"shap_{col}\" for col in X.columns]\n",
    "        shap_df = pd.DataFrame(shap_vals, columns=shap_cols, index=X.index)\n",
    "        result = pd.concat([X, shap_df], axis=1)\n",
    "\n",
    "        if self.config.get('add_base_value', False):\n",
    "            if hasattr(explainer, 'expected_value'):\n",
    "                base_val = explainer.expected_value\n",
    "                if isinstance(base_val, list):\n",
    "                    class_index = self.config.get('class_index', 0)\n",
    "                    base_val = base_val[class_index]\n",
    "                result['shap_base_value'] = base_val\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _lime_explain(self, X, model):\n",
    "        \"\"\"محاسبه توضیحات LIME برای هر نمونه.\"\"\"\n",
    "        try:\n",
    "            from lime import lime_tabular\n",
    "        except ImportError:\n",
    "            raise ImportError(\"lime package is required for LIMEExplainer.\")\n",
    "\n",
    "        mode = self.config.get('mode', 'classification')\n",
    "        feature_names = list(X.columns)\n",
    "        class_names = self.config.get('class_names', None)\n",
    "\n",
    "        explainer = lime_tabular.LimeTabularExplainer(\n",
    "            X.values,\n",
    "            feature_names=feature_names,\n",
    "            class_names=class_names,\n",
    "            mode=mode,\n",
    "            **self.config.get('explainer_kwargs', {})\n",
    "        )\n",
    "\n",
    "        if mode == 'classification':\n",
    "            predict_fn = model.predict_proba\n",
    "        else:\n",
    "            predict_fn = model.predict\n",
    "\n",
    "        explanations = []\n",
    "        for idx in range(len(X)):\n",
    "            exp = explainer.explain_instance(\n",
    "                X.iloc[idx].values,\n",
    "                predict_fn,\n",
    "                num_features=self.config.get('num_features', len(feature_names)),\n",
    "                **self.config.get('explain_instance_kwargs', {})\n",
    "            )\n",
    "            exp_dict = {f\"lime_{feat}\": weight for feat, weight in exp.as_list()}\n",
    "            explanations.append(exp_dict)\n",
    "\n",
    "        lime_df = pd.DataFrame(explanations, index=X.index).fillna(0)\n",
    "        result = pd.concat([X, lime_df], axis=1)\n",
    "        return result\n",
    "\n",
    "    def _feature_importance(self, X, model):\n",
    "        \"\"\"محاسبه اهمیت ویژگی‌ها از مدل.\"\"\"\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            coef = model.coef_\n",
    "            if coef.ndim > 1:\n",
    "                agg = self.config.get('coef_aggregation', 'mean')\n",
    "                if agg == 'mean':\n",
    "                    importances = np.mean(np.abs(coef), axis=0)\n",
    "                elif agg == 'max':\n",
    "                    importances = np.max(np.abs(coef), axis=0)\n",
    "                else:\n",
    "                    importances = np.abs(coef[0])\n",
    "            else:\n",
    "                importances = np.abs(coef)\n",
    "        else:\n",
    "            raise AttributeError(\"Model does not have feature_importances_ or coef_.\")\n",
    "\n",
    "        importance_dict = {f\"importance_{col}\": imp for col, imp in zip(X.columns, importances)}\n",
    "        importance_row = pd.Series(importance_dict)\n",
    "        importance_df = pd.DataFrame([importance_row] * len(X), index=X.index)\n",
    "        result = pd.concat([X, importance_df], axis=1)\n",
    "        return result\n",
    "\n",
    "    def _partial_dependence(self, X, model):\n",
    "        \"\"\"محاسبه وابستگی جزئی برای ویژگی‌های مشخص.\"\"\"\n",
    "        try:\n",
    "            from sklearn.inspection import partial_dependence\n",
    "        except ImportError:\n",
    "            raise ImportError(\"sklearn.inspection is required for PartialDependence.\")\n",
    "\n",
    "        features = self.config.get('features')\n",
    "        if features is None:\n",
    "            raise ValueError(\"For PartialDependence, 'features' must be specified in config (list of feature names or indices).\")\n",
    "\n",
    "        if all(isinstance(f, str) for f in features):\n",
    "            feature_indices = [X.columns.get_loc(f) for f in features]\n",
    "        else:\n",
    "            feature_indices = features\n",
    "\n",
    "        kind = self.config.get('kind', 'average')\n",
    "        pdp_results = partial_dependence(\n",
    "            model, X, features=feature_indices,\n",
    "            kind=kind, **self.config.get('pdp_kwargs', {})\n",
    "        )\n",
    "\n",
    "        if kind == 'individual':\n",
    "            # در حالت individual، مقادیر میانگین را برمی‌گردانیم (ساده‌سازی)\n",
    "            pdp_avg = partial_dependence(model, X, features=feature_indices, kind='average')['average']\n",
    "            pdp_dict = {}\n",
    "            for i, feat in enumerate(features):\n",
    "                pdp_dict[f\"pdp_{feat}_avg\"] = pdp_avg[i]\n",
    "            pdp_df = pd.DataFrame([pdp_dict] * len(X), index=X.index)\n",
    "        else:\n",
    "            pdp_avg = pdp_results['average']\n",
    "            pdp_dict = {}\n",
    "            for i, feat in enumerate(features):\n",
    "                pdp_dict[f\"pdp_{feat}\"] = pdp_avg[i]\n",
    "            pdp_df = pd.DataFrame([pdp_dict] * len(X), index=X.index)\n",
    "\n",
    "        result = pd.concat([X, pdp_df], axis=1)\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
